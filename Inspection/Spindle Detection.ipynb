{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file C:\\Users\\Sammauel\\Documents\\csc59969\\EEG\\EEG-Visualization\\Inspection\\suj28_l2nap_day1.fif...\n",
      "This filename (C:\\Users\\Sammauel\\Documents\\csc59969\\EEG\\EEG-Visualization\\Inspection\\suj28_l2nap_day1.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz or raw_tsss.fif.gz\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 61)  idle\n",
      "    Range : 0 ... 1807649 =      0.000 ...  1807.649 secs\n",
      "Ready.\n",
      "Reading 0 ... 1807649  =      0.000 ...  1807.649 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4a11b747854a>:199: RuntimeWarning: This filename (C:\\Users\\Sammauel\\Documents\\csc59969\\EEG\\EEG-Visualization\\Inspection\\suj28_l2nap_day1.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz or raw_tsss.fif.gz\n",
      "  raw=mne.io.read_raw_fif(file_to_read,add_eeg_ref=False,preload=True)#loading the preprocessed data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'Annotation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4a11b747854a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_raw_fif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_to_read\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madd_eeg_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#loading the preprocessed data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0mchannelList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'F3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'F4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'C3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'C4'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'O1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'O2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m \u001b[0mtime_find\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean_peak_power\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDuration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmpl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauto_proba\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauto_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_Onest_Amplitude_Duration_of_spindles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchannelList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmoving_window_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlower_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhigher_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msyn_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh_bound\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Onset\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtime_find\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Amplitude\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmean_peak_power\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Duration'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mDuration\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Annotation'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'auto spindle'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a11b747854a>\u001b[0m in \u001b[0;36mget_Onest_Amplitude_Duration_of_spindles\u001b[0;34m(raw, channelList, annotations, moving_window_size, lower_threshold, syn_channels, l_bound, h_bound, tol, higher_threshold, front, back, sleep_stage, proba, validation_windowsize)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mtemp_time_find\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mtemp_mean_peak_power\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mtemp_duration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[1;31m# seperate out stage 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mstages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mannotations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAnnotation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage_check\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mOn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[0mOff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mstage_on_off\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOnset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOnset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'Annotation'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script serves as an example of how spindles are classified using filer based and thresholding approachs\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.stats import hmean,trim_mean\n",
    "import mne\n",
    "\n",
    "# define functions used in this pipeline\n",
    "def window_rms(a, window_size):\n",
    "    \"\"\"\n",
    "    a: numpy array of sample data\n",
    "    window_size: size of convolution kernel\n",
    "    \"\"\"\n",
    "    a2 = np.power(a,2)# sqaure all data points\n",
    "    window = scipy.signal.gaussian(window_size,(window_size/.68)/2)# apply gaussian window with length of window_size samples\n",
    "    return np.sqrt(np.convolve(a2, window, 'same')/len(a2)) * 1e2 # convolve along entire data and return root mean sqaures with the same length of the sample data\n",
    "def trimmed_std(data,percentile):\n",
    "    temp=data.copy()\n",
    "    temp.sort()\n",
    "    percentile = percentile / 2\n",
    "    low = int(percentile * len(temp))\n",
    "    high = int((1. - percentile) * len(temp))\n",
    "    return temp[low:high].std(ddof=0)\n",
    "\n",
    "def get_Onest_Amplitude_Duration_of_spindles(raw,channelList,\n",
    "                                        annotations=None,\n",
    "                                        moving_window_size=200,\n",
    "                                        lower_threshold=.9,\n",
    "                                        syn_channels=3,\n",
    "                                        l_bound=0.5,h_bound=2,\n",
    "                                        tol=1,higher_threshold=3.5,\n",
    "                                        front=300,back=100,\n",
    "                                        sleep_stage=True,\n",
    "                                        proba=False,\n",
    "                                        validation_windowsize=3):\n",
    "    \"\"\"\n",
    "    raw: data after preprocessing\n",
    "    channelList: channel list of interest, and in this study we use       'F3','F4','C3','C4','O1','O2'\n",
    "    annotations: pandas DataFrame object containing manual annotations, such as sleep stages, spindle locations.\n",
    "    moving_window_size: size of the moving window for convolved root mean square computation. It should work better when it is the sampling frequency, which, in this case is 500 (we downsample subjects with 1000 Hz sampling rate). \n",
    "    lower_threshold: highpass threshold for spindle detection: decision making = trimmed_mean + lower_T * trimmed_std\n",
    "    higher_threshold: lowpass threshold for spindle detection: decision making = trimmed_mean + higher_T * trimmed_std\n",
    "    syn_channels: criteria for selecting spindles: at least # of channels have spindle instance and also in the mean channel\n",
    "    l_bound: low boundary for duration of a spindle instance\n",
    "    h_bound: high boundary for duration of a spindle instance\n",
    "    tol : tolerance for determing spindles (criteria in time)\n",
    "    front : First few seconds of recordings that we are not interested because there might be artifacts, or it is confirmed subjects could not fall asleep within such a short period\n",
    "    back : last few seconds of recordings that we are not interested due to the recording procedures\n",
    "    \"\"\"\n",
    "    # process the data without any other information\n",
    "    time=np.linspace(0,raw.last_samp/raw.info['sfreq'],raw._data[0,:].shape[0])\n",
    "    RMS = np.zeros((len(channelList),raw._data[0,:].shape[0]))\n",
    "    peak_time={} #preallocate\n",
    "    sfreq=raw.info['sfreq']\n",
    "    mph,mpl = {},{}\n",
    "\n",
    "    for ii, names in enumerate(channelList):\n",
    "\n",
    "        peak_time[names]=[]\n",
    "        segment,_ = raw[ii,:]\n",
    "        RMS[ii,:] = window_rms(segment[0,:],moving_window_size) \n",
    "        mph[names] = trim_mean(RMS[ii,int(front*sfreq):-int(back*sfreq)],0.05) + lower_threshold * trimmed_std(RMS[ii,:],0.05) \n",
    "        mpl[names] = trim_mean(RMS[ii,int(front*sfreq):-int(back*sfreq)],0.05) + higher_threshold * trimmed_std(RMS[ii,:],0.05)\n",
    "        pass_ = RMS[ii,:] > mph[names]#should be greater than then mean not the threshold to compute duration\n",
    "\n",
    "        up = np.where(np.diff(pass_.astype(int))>0)\n",
    "        down = np.where(np.diff(pass_.astype(int))<0)\n",
    "        up = up[0]\n",
    "        down = down[0]\n",
    "        #######key to idenfity segments that goes beyond the lower threshold########\n",
    "        #print(down[0],up[0])\n",
    "        if down[0] < up[0]:\n",
    "            down = down[1:]\n",
    "        #print(down[0],up[0])\n",
    "        #############################\n",
    "        if (up.shape > down.shape) or (up.shape < down.shape):\n",
    "            size = np.min([up.shape,down.shape])\n",
    "            up = up[:size]\n",
    "            down = down[:size]\n",
    "        C = np.vstack((up,down))\n",
    "        for pairs in C.T:\n",
    "            if l_bound < (time[pairs[1]] - time[pairs[0]]) < h_bound:\n",
    "                SegmentForPeakSearching = RMS[ii,pairs[0]:pairs[1]]\n",
    "                if np.max(SegmentForPeakSearching) < mpl[names]:\n",
    "                    temp_temp_time = time[pairs[0]:pairs[1]]\n",
    "                    ints_temp = np.argmax(SegmentForPeakSearching)\n",
    "                    peak_time[names].append(temp_temp_time[ints_temp])\n",
    "    peak_time['mean']=[];peak_at=[];duration=[]\n",
    "    RMS_mean=hmean(RMS)\n",
    "    # apply the same algorithm to the mean of the RMSs\n",
    "    mph['mean'] = trim_mean(RMS_mean[int(front*sfreq):-int(back*sfreq)],0.05) + lower_threshold * trimmed_std(RMS_mean,0.05)\n",
    "    mpl['mean'] = trim_mean(RMS_mean[int(front*sfreq):-int(back*sfreq)],0.05) + higher_threshold * trimmed_std(RMS_mean,0.05)\n",
    "    pass_ =RMS_mean > mph['mean']\n",
    "    up = np.where(np.diff(pass_.astype(int))>0)\n",
    "    down= np.where(np.diff(pass_.astype(int))<0)\n",
    "    up = up[0]\n",
    "    down = down[0]\n",
    "    ###############################\n",
    "    #print(down[0],up[0])\n",
    "    if down[0] < up[0]:\n",
    "        down = down[1:]\n",
    "    #print(down[0],up[0])\n",
    "    #############################\n",
    "    if (up.shape > down.shape) or (up.shape < down.shape):\n",
    "        size = np.min([up.shape,down.shape])\n",
    "        up = up[:size]\n",
    "        down = down[:size]\n",
    "    C = np.vstack((up,down))\n",
    "    for pairs in C.T:\n",
    "        \n",
    "        if l_bound < (time[pairs[1]] - time[pairs[0]]) < h_bound:\n",
    "            SegmentForPeakSearching = RMS_mean[pairs[0]:pairs[1],]\n",
    "            if np.max(SegmentForPeakSearching)< mpl['mean']:\n",
    "                temp_time = time[pairs[0]:pairs[1]]\n",
    "                ints_temp = np.argmax(SegmentForPeakSearching)\n",
    "                peak_time['mean'].append(temp_time[ints_temp])\n",
    "                peak_at.append(SegmentForPeakSearching[ints_temp])\n",
    "                duration_temp = time[pairs[1]] - time[pairs[0]]\n",
    "                duration.append(duration_temp) \n",
    "    time_find=[];mean_peak_power=[];Duration=[];\n",
    "    for item,PEAK,duration_time in zip(peak_time['mean'],peak_at,duration):\n",
    "        temp_timePoint=[]\n",
    "        for ii, names in enumerate(channelList):\n",
    "            try:\n",
    "                temp_timePoint.append(min(enumerate(peak_time[names]), key=lambda x: abs(x[1]-item))[1])\n",
    "            except:\n",
    "                temp_timePoint.append(item + 2)\n",
    "        try:\n",
    "            if np.sum((abs(np.array(temp_timePoint) - item)<tol).astype(int))>=syn_channels:\n",
    "                time_find.append(float(item))\n",
    "                mean_peak_power.append(PEAK)\n",
    "                Duration.append(duration_time)\n",
    "        except:\n",
    "            pass\n",
    "    ############ the end of the processing in which no other inputs ##\n",
    "    #### update the spindles we found if we want to add information of sleep stages ######\n",
    "    if sleep_stage:\n",
    "        \n",
    "        temp_time_find=[];temp_mean_peak_power=[];temp_duration=[];\n",
    "        # seperate out stage 2\n",
    "        stages = annotations[annotations.Annotation.apply(stage_check)]\n",
    "        On = stages[::2];Off = stages[1::2]\n",
    "        stage_on_off = list(zip(On.Onset.values, Off.Onset.values))\n",
    "        if abs(np.diff(stage_on_off[0]) - 30) < 2:\n",
    "            pass\n",
    "        else:\n",
    "            On = stages[1::2];Off = stages[::2]\n",
    "            stage_on_off = list(zip(On.Onset.values[1:], Off.Onset.values[2:]))\n",
    "        for single_time_find, single_mean_peak_power, single_duration in zip(time_find,mean_peak_power,Duration):\n",
    "            for on_time,off_time in stage_on_off:\n",
    "                if intervalCheck([on_time,off_time],single_time_find,tol=tol):\n",
    "                    temp_time_find.append(single_time_find)\n",
    "                    temp_mean_peak_power.append(single_mean_peak_power)\n",
    "                    temp_duration.append(single_duration)\n",
    "        time_find=temp_time_find;mean_peak_power=temp_mean_peak_power;Duration=temp_duration\n",
    "    \n",
    "    ####### decision function based on spindles we have just found ####\n",
    "    \"\"\"\n",
    "    A single floating representation is computed based on the validation window size (say 3 seconds), and information like peak power densities and peak frequencies are added to the feature space.\n",
    "    We fit the standandardized features with the labels (spindles found by the automated pipeline)\n",
    "    A prediction probability is computed using scikit-learn::logisticregression\n",
    "    \"\"\"\n",
    "    decision_features=None\n",
    "    if proba:\n",
    "        result = pd.DataFrame({'Onset':time_find,'Duration':Duration,'Annotation':['spindle']*len(Duration)})     \n",
    "        auto_label,_ = discritized_onset_label_auto(raw,result,validation_windowsize)\n",
    "        events = mne.make_fixed_length_events(raw,id=1,start=0,duration=validation_windowsize)\n",
    "        epochs = mne.Epochs(raw,events,event_id=1,tmin=0,tmax=validation_windowsize,preload=True)\n",
    "        data = epochs.get_data()[:,:,:-1]\n",
    "        full_prop=[]        \n",
    "        for d in data:    \n",
    "            temp_p=[]\n",
    "            #fig,ax = plt.subplots(nrows=2,ncols=3,figsize=(8,8))\n",
    "            for ii,(name) in enumerate(zip(channelList)):#,ax.flatten())):\n",
    "                rms = window_rms(d[ii,:],500)\n",
    "                l = trim_mean(rms,0.05) + lower_threshold * trimmed_std(rms,0.05)\n",
    "                h = trim_mean(rms,0.05) + higher_threshold * trimmed_std(rms,0.05)\n",
    "                prop = (sum(rms>l)+sum(rms<h))/(sum(rms<h) - sum(rms<l))\n",
    "                if np.isinf(prop):\n",
    "                    prop = (sum(rms>l)+sum(rms<h))\n",
    "                temp_p.append(prop)\n",
    "                \n",
    "            \n",
    "            full_prop.append(temp_p)\n",
    "        psds,freq = mne.time_frequency.psd_multitaper(epochs,fmin=11,fmax=16,tmin=0,tmax=3,low_bias=True,)\n",
    "        psds = 10* np.log10(psds)\n",
    "        features = pd.DataFrame(np.concatenate((np.array(full_prop),psds.max(2),freq[np.argmax(psds,2)]),1))\n",
    "        decision_features = StandardScaler().fit_transform(features.values,auto_label)\n",
    "        clf = LogisticRegressionCV(Cs=np.logspace(-4,6,11),cv=5,tol=1e-7,max_iter=int(1e7))\n",
    "        clf.fit(decision_features,auto_label)\n",
    "        auto_proba=clf.predict_proba(decision_features)[:,-1]\n",
    "    return time_find,mean_peak_power,Duration,mph,mpl,auto_proba,auto_label\n",
    "\n",
    "\n",
    "file_to_read = \"suj28_l2nap_day1.fif\"\n",
    "raw=mne.io.read_raw_fif(file_to_read,add_eeg_ref=False,preload=True)#loading the preprocessed data\n",
    "channelList = ['F3','F4','C3','C4','O1','O2']\n",
    "time_find,mean_peak_power,Duration,mph,mpl,auto_proba,auto_label=get_Onest_Amplitude_Duration_of_spindles(raw,channelList,moving_window_size=500,lower_threshold=.9,higher_threshold=3,syn_channels=3,l_bound=0.5,h_bound=2,tol=1)\n",
    "result = pd.DataFrame({\"Onset\":time_find,\"Amplitude\":mean_peak_power,'Duration':Duration})\n",
    "result['Annotation'] = 'auto spindle'\n",
    "result = result[result.Onset < (raw.last_samp/raw.info['sfreq'] - 100)]\n",
    "result = result[result.Onset > 100]\n",
    "result.to_csv(filename,index_col=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
