{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file C:\\Users\\Sammauel\\Documents\\csc59969\\EEG\\EEG-Visualization\\Inspection\\suj28_l2nap_day1.fif...\n",
      "This filename (C:\\Users\\Sammauel\\Documents\\csc59969\\EEG\\EEG-Visualization\\Inspection\\suj28_l2nap_day1.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz or raw_tsss.fif.gz\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 61)  idle\n",
      "    Range : 0 ... 1807649 =      0.000 ...  1807.649 secs\n",
      "Ready.\n",
      "Reading 0 ... 1807649  =      0.000 ...  1807.649 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f2876b859daa>:242: RuntimeWarning: This filename (C:\\Users\\Sammauel\\Documents\\csc59969\\EEG\\EEG-Visualization\\Inspection\\suj28_l2nap_day1.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz or raw_tsss.fif.gz\n",
      "  raw=mne.io.read_raw_fif(file_to_read,add_eeg_ref=False,preload=True)#loading the preprocessed data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band-pass filtering from 11 - 16 Hz\n",
      "Multiple deprecated filter parameters were used:\n",
      "phase in 0.13 is \"zero-double\" but will change to \"zero\" in 0.14\n",
      "fir_window in 0.13 is \"hann\" but will change to \"hamming\" in 0.14\n",
      "lower transition bandwidth in 0.13 is 0.5 Hz but will change to \"auto\" in 0.14\n",
      "upper transition bandwidth in 0.13 is 0.5 Hz but will change to \"auto\" in 0.14\n",
      "The default filter length in 0.13 is \"10s\" but will change to \"auto\" in 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f2876b859daa>:244: DeprecationWarning: Multiple deprecated filter parameters were used:\n",
      "phase in 0.13 is \"zero-double\" but will change to \"zero\" in 0.14\n",
      "fir_window in 0.13 is \"hann\" but will change to \"hamming\" in 0.14\n",
      "lower transition bandwidth in 0.13 is 0.5 Hz but will change to \"auto\" in 0.14\n",
      "upper transition bandwidth in 0.13 is 0.5 Hz but will change to \"auto\" in 0.14\n",
      "The default filter length in 0.13 is \"10s\" but will change to \"auto\" in 0.14\n",
      "  raw.filter(11,16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "add_eeg_ref defaults to True in 0.13, will default to False in 0.14, and will be removed in 0.15. We recommend to use add_eeg_ref=False and set_eeg_reference() instead.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Loading data for 469 events and 1501 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f2876b859daa>:214: DeprecationWarning: add_eeg_ref defaults to True in 0.13, will default to False in 0.14, and will be removed in 0.15. We recommend to use add_eeg_ref=False and set_eeg_reference() instead.\n",
      "  epochs = mne.Epochs(raw,events,event_id=1,tmin=0,tmax=validation_windowsize,preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script serves as an example of how spindles are classified using filer based and thresholding approachs\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from scipy.stats import hmean,trim_mean\n",
    "import mne\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# define functions used in this pipeline\n",
    "def window_rms(a, window_size):\n",
    "    \"\"\"\n",
    "    a: numpy array of sample data\n",
    "    window_size: size of convolution kernel\n",
    "    \"\"\"\n",
    "    a2 = np.power(a,2)# sqaure all data points\n",
    "    window = signal.gaussian(window_size,(window_size/.68)/2)# apply gaussian window with length of window_size samples\n",
    "    return np.sqrt(np.convolve(a2, window, 'same')/len(a2)) * 1e2 # convolve along entire data and return root mean sqaures with the same length of the sample data\n",
    "def trimmed_std(data,percentile):\n",
    "    temp=data.copy()\n",
    "    temp.sort()\n",
    "    percentile = percentile / 2\n",
    "    low = int(percentile * len(temp))\n",
    "    high = int((1. - percentile) * len(temp))\n",
    "    return temp[low:high].std(ddof=0)\n",
    "def stage_check(x):\n",
    "    import re\n",
    "    if re.compile('2',re.IGNORECASE).search(x):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def intervalCheck(a,b,tol=0):#a is an array and b is a point\n",
    "    return a[0]-tol <= b <= a[1]+tol\n",
    "def spindle_comparison(time_interval,spindle,spindle_duration,spindle_duration_fix=True):\n",
    "    \"\"\"\n",
    "    Compare a spindle sample with the 2nd stage sleep intervals\n",
    "    \"\"\"\n",
    "    if spindle_duration_fix:\n",
    "        spindle_start = spindle - 0.5\n",
    "        spindle_end   = spindle + 1.5\n",
    "        a =  np.logical_or((intervalCheck(time_interval,spindle_start)),\n",
    "                           (intervalCheck(time_interval,spindle_end)))\n",
    "        return a\n",
    "    else:\n",
    "        spindle_start = spindle - spindle_duration/2.\n",
    "        spindle_end   = spindle + spindle_duration/2.\n",
    "        a = np.logical_or((intervalCheck(time_interval,spindle_start)),\n",
    "                           (intervalCheck(time_interval,spindle_end)))\n",
    "        return a\n",
    "def discritized_onset_label_auto(raw,df,spindle_segment,front=300,back=100):\n",
    "    \"\"\"\n",
    "    Discritizing continoues time float to 0 or 1 segments with 3 seconds long\n",
    "    \"\"\"\n",
    "    spindle_duration = df['Duration'].values\n",
    "    discritized_continuous_time = np.arange(front,raw.times[-1]-back,step=spindle_segment)\n",
    "    discritized_time_intervals = np.vstack((discritized_continuous_time[:-1],discritized_continuous_time[1:]))\n",
    "    discritized_time_intervals = np.array(discritized_time_intervals).T\n",
    "    discritized_time_to_zero_one_labels = np.zeros(len(discritized_time_intervals))\n",
    "    for jj,(time_interval_1,time_interval_2) in enumerate(discritized_time_intervals):\n",
    "        time_interval = [time_interval_1,time_interval_2]\n",
    "        for kk,spindle in enumerate(df['Onset']):\n",
    "            if spindle_comparison(time_interval,spindle,spindle_duration[kk],spindle_duration_fix=False):\n",
    "                discritized_time_to_zero_one_labels[jj] = 1\n",
    "    return discritized_time_to_zero_one_labels,discritized_time_intervals\n",
    "def get_Onest_Amplitude_Duration_of_spindles(raw,channelList,\n",
    "                                        annotations=None,\n",
    "                                        moving_window_size=200,\n",
    "                                        lower_threshold=.9,\n",
    "                                        syn_channels=3,\n",
    "                                        l_bound=0.5,h_bound=2,\n",
    "                                        tol=1,higher_threshold=3.5,\n",
    "                                        front=300,back=100,\n",
    "                                        sleep_stage=True,\n",
    "                                        proba=True,\n",
    "                                        validation_windowsize=3,\n",
    "                                        l_freq=11,h_freq=16):\n",
    "    \"\"\"\n",
    "    raw: data after preprocessing\n",
    "    channelList: channel list of interest, and in this study we use       'F3','F4','C3','C4','O1','O2'\n",
    "    annotations: pandas DataFrame object containing manual annotations, such as sleep stages, spindle locations.\n",
    "    moving_window_size: size of the moving window for convolved root mean square computation. It should work better when it is the sampling frequency, which, in this case is 500 (we downsample subjects with 1000 Hz sampling rate). \n",
    "    lower_threshold: highpass threshold for spindle detection: decision making = trimmed_mean + lower_T * trimmed_std\n",
    "    higher_threshold: lowpass threshold for spindle detection: decision making = trimmed_mean + higher_T * trimmed_std\n",
    "    syn_channels: criteria for selecting spindles: at least # of channels have spindle instance and also in the mean channel\n",
    "    l_bound: low boundary for duration of a spindle instance\n",
    "    h_bound: high boundary for duration of a spindle instance\n",
    "    tol : tolerance for determing spindles (criteria in time)\n",
    "    front : First few seconds of recordings that we are not interested because there might be artifacts, or it is confirmed subjects could not fall asleep within such a short period\n",
    "    back : last few seconds of recordings that we are not interested due to the recording procedures\n",
    "    \"\"\"\n",
    "    # process the data without any other information\n",
    "    time=np.linspace(0,raw.last_samp/raw.info['sfreq'],raw._data[0,:].shape[0])\n",
    "    RMS = np.zeros((len(channelList),raw._data[0,:].shape[0]))\n",
    "    peak_time={} #preallocate\n",
    "    sfreq=raw.info['sfreq']\n",
    "    mph,mpl = {},{}\n",
    "\n",
    "    for ii, names in enumerate(channelList):\n",
    "\n",
    "        peak_time[names]=[]\n",
    "        segment,_ = raw[ii,:]\n",
    "        RMS[ii,:] = window_rms(segment[0,:],moving_window_size) \n",
    "        mph[names] = trim_mean(RMS[ii,int(front*sfreq):-int(back*sfreq)],0.05) + lower_threshold * trimmed_std(RMS[ii,:],0.05) \n",
    "        mpl[names] = trim_mean(RMS[ii,int(front*sfreq):-int(back*sfreq)],0.05) + higher_threshold * trimmed_std(RMS[ii,:],0.05)\n",
    "        pass_ = RMS[ii,:] > mph[names]#should be greater than then mean not the threshold to compute duration\n",
    "\n",
    "        up = np.where(np.diff(pass_.astype(int))>0)\n",
    "        down = np.where(np.diff(pass_.astype(int))<0)\n",
    "        up = up[0]\n",
    "        down = down[0]\n",
    "        #######key to idenfity segments that goes beyond the lower threshold########\n",
    "        #print(down[0],up[0])\n",
    "        if down[0] < up[0]:\n",
    "            down = down[1:]\n",
    "        #print(down[0],up[0])\n",
    "        #############################\n",
    "        if (up.shape > down.shape) or (up.shape < down.shape):\n",
    "            size = np.min([up.shape,down.shape])\n",
    "            up = up[:size]\n",
    "            down = down[:size]\n",
    "        C = np.vstack((up,down))\n",
    "        for pairs in C.T:\n",
    "            if l_bound < (time[pairs[1]] - time[pairs[0]]) < h_bound:\n",
    "                SegmentForPeakSearching = RMS[ii,pairs[0]:pairs[1]]\n",
    "                if np.max(SegmentForPeakSearching) < mpl[names]:\n",
    "                    temp_temp_time = time[pairs[0]:pairs[1]]\n",
    "                    ints_temp = np.argmax(SegmentForPeakSearching)\n",
    "                    peak_time[names].append(temp_temp_time[ints_temp])\n",
    "    peak_time['mean']=[];peak_at=[];duration=[]\n",
    "    RMS_mean=hmean(RMS)\n",
    "    # apply the same algorithm to the mean of the RMSs\n",
    "    mph['mean'] = trim_mean(RMS_mean[int(front*sfreq):-int(back*sfreq)],0.05) + lower_threshold * trimmed_std(RMS_mean,0.05)\n",
    "    mpl['mean'] = trim_mean(RMS_mean[int(front*sfreq):-int(back*sfreq)],0.05) + higher_threshold * trimmed_std(RMS_mean,0.05)\n",
    "    pass_ =RMS_mean > mph['mean']\n",
    "    up = np.where(np.diff(pass_.astype(int))>0)\n",
    "    down= np.where(np.diff(pass_.astype(int))<0)\n",
    "    up = up[0]\n",
    "    down = down[0]\n",
    "    ###############################\n",
    "    #print(down[0],up[0])\n",
    "    if down[0] < up[0]:\n",
    "        down = down[1:]\n",
    "    #print(down[0],up[0])\n",
    "    #############################\n",
    "    if (up.shape > down.shape) or (up.shape < down.shape):\n",
    "        size = np.min([up.shape,down.shape])\n",
    "        up = up[:size]\n",
    "        down = down[:size]\n",
    "    C = np.vstack((up,down))\n",
    "    for pairs in C.T:\n",
    "        \n",
    "        if l_bound < (time[pairs[1]] - time[pairs[0]]) < h_bound:\n",
    "            SegmentForPeakSearching = RMS_mean[pairs[0]:pairs[1],]\n",
    "            if np.max(SegmentForPeakSearching)< mpl['mean']:\n",
    "                temp_time = time[pairs[0]:pairs[1]]\n",
    "                ints_temp = np.argmax(SegmentForPeakSearching)\n",
    "                peak_time['mean'].append(temp_time[ints_temp])\n",
    "                peak_at.append(SegmentForPeakSearching[ints_temp])\n",
    "                duration_temp = time[pairs[1]] - time[pairs[0]]\n",
    "                duration.append(duration_temp) \n",
    "    time_find=[];mean_peak_power=[];Duration=[];\n",
    "    for item,PEAK,duration_time in zip(peak_time['mean'],peak_at,duration):\n",
    "        temp_timePoint=[]\n",
    "        for ii, names in enumerate(channelList):\n",
    "            try:\n",
    "                temp_timePoint.append(min(enumerate(peak_time[names]), key=lambda x: abs(x[1]-item))[1])\n",
    "            except:\n",
    "                temp_timePoint.append(item + 2)\n",
    "        try:\n",
    "            if np.sum((abs(np.array(temp_timePoint) - item)<tol).astype(int))>=syn_channels:\n",
    "                time_find.append(float(item))\n",
    "                mean_peak_power.append(PEAK)\n",
    "                Duration.append(duration_time)\n",
    "        except:\n",
    "            pass\n",
    "    ############ the end of the processing in which no other inputs ##\n",
    "    #### update the spindles we found if we want to add information of sleep stages ######\n",
    "    if sleep_stage:\n",
    "        \n",
    "        temp_time_find=[];temp_mean_peak_power=[];temp_duration=[];\n",
    "        # seperate out stage 2\n",
    "        stages = annotations[annotations.Annotation.apply(stage_check)]\n",
    "        On = stages[::2];Off = stages[1::2]\n",
    "        stage_on_off = list(zip(On.Onset.values, Off.Onset.values))\n",
    "        if abs(np.diff(stage_on_off[0]) - 30) < 2:\n",
    "            pass\n",
    "        else:\n",
    "            On = stages[1::2];Off = stages[::2]\n",
    "            stage_on_off = list(zip(On.Onset.values[1:], Off.Onset.values[2:]))\n",
    "        for single_time_find, single_mean_peak_power, single_duration in zip(time_find,mean_peak_power,Duration):\n",
    "            for on_time,off_time in stage_on_off:\n",
    "                if intervalCheck([on_time,off_time],single_time_find,tol=tol):\n",
    "                    temp_time_find.append(single_time_find)\n",
    "                    temp_mean_peak_power.append(single_mean_peak_power)\n",
    "                    temp_duration.append(single_duration)\n",
    "        time_find=temp_time_find;mean_peak_power=temp_mean_peak_power;Duration=temp_duration\n",
    "    \n",
    "    ####### decision function based on spindles we have just found ####\n",
    "    \"\"\"\n",
    "    A single floating representation is computed based on the validation window size (say 3 seconds), and information like peak power densities and peak frequencies are added to the feature space.\n",
    "    We fit the standandardized features with the labels (spindles found by the automated pipeline)\n",
    "    A prediction probability is computed using scikit-learn::logisticregression\n",
    "    \"\"\"\n",
    "    decision_features=None;auto_proba=None;auto_label=None\n",
    "    if proba:\n",
    "        result = pd.DataFrame({'Onset':time_find,'Duration':Duration,'Annotation':['spindle']*len(Duration)})     \n",
    "        auto_label,_ = discritized_onset_label_auto(raw,result,validation_windowsize)\n",
    "        events = mne.make_fixed_length_events(raw,id=1,start=front,stop=raw.times[-1]-back,duration=validation_windowsize)\n",
    "        epochs = mne.Epochs(raw,events,event_id=1,tmin=0,tmax=validation_windowsize,preload=True)\n",
    "        data = epochs.get_data()[:,:,:-1]\n",
    "        full_prop=[]        \n",
    "        for d in data:    \n",
    "            temp_p=[]\n",
    "            #fig,ax = plt.subplots(nrows=2,ncols=3,figsize=(8,8))\n",
    "            for ii,(name) in enumerate(zip(channelList)):#,ax.flatten())):\n",
    "                rms = window_rms(d[ii,:],500)\n",
    "                l = trim_mean(rms,0.05) + lower_threshold * trimmed_std(rms,0.05)\n",
    "                h = trim_mean(rms,0.05) + higher_threshold * trimmed_std(rms,0.05)\n",
    "                prop = (sum(rms>l)+sum(rms<h))/(sum(rms<h) - sum(rms<l))\n",
    "                if np.isinf(prop):\n",
    "                    prop = (sum(rms>l)+sum(rms<h))\n",
    "                temp_p.append(prop)\n",
    "                \n",
    "            \n",
    "            full_prop.append(temp_p)\n",
    "        psds,freq = mne.time_frequency.psd_multitaper(epochs,fmin=l_freq,fmax=h_freq,tmin=0,tmax=3,low_bias=True,)\n",
    "        psds = 10* np.log10(psds)\n",
    "        features = pd.DataFrame(np.concatenate((np.array(full_prop),psds.max(2),freq[np.argmax(psds,2)]),1))\n",
    "        decision_features = StandardScaler().fit_transform(features.values,auto_label)\n",
    "        clf = LogisticRegressionCV(Cs=np.logspace(-4,6,11),cv=5,tol=1e-7,max_iter=int(1e7))\n",
    "        clf.fit(decision_features,auto_label)\n",
    "        auto_proba=clf.predict_proba(decision_features)[:,-1]\n",
    "    return time_find,mean_peak_power,Duration,mph,mpl,auto_proba,auto_label\n",
    "\n",
    "\n",
    "file_to_read = \"suj28_l2nap_day1.fif\"\n",
    "raw=mne.io.read_raw_fif(file_to_read,add_eeg_ref=False,preload=True)#loading the preprocessed data\n",
    "raw.resample(500)\n",
    "raw.filter(11,16)\n",
    "channelList = ['F3','F4','C3','C4','O1','O2']\n",
    "annotation = pd.read_csv('suj28_nap_day1_edited_annotations.txt')\n",
    "time_find,mean_peak_power,Duration,mph,mpl,auto_proba,auto_label=get_Onest_Amplitude_Duration_of_spindles(raw,\n",
    "                                                                        channelList,\n",
    "                                                                        moving_window_size=500,\n",
    "                                                                        annotations=annotation,\n",
    "                                                                        lower_threshold=.4,\n",
    "                                                                        higher_threshold=3.4,\n",
    "                                                                        syn_channels=3,\n",
    "                                                                        l_bound=0.5,\n",
    "                                                                        h_bound=2,\n",
    "                                                                        sleep_stage=True,\n",
    "                                                                        proba=True,\n",
    "                                                                        tol=1)\n",
    "\n",
    "result = pd.DataFrame({\"Onset\":time_find,\"Amplitude\":mean_peak_power,'Duration':Duration})\n",
    "result['Annotation'] = 'auto spindle'\n",
    "\n",
    "result.to_csv('spindle_detection_suj28.txt',index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Onset</th>\n",
       "      <th>Annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.905970</td>\n",
       "      <td>1.102</td>\n",
       "      <td>695.810</td>\n",
       "      <td>auto spindle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.556632</td>\n",
       "      <td>0.804</td>\n",
       "      <td>707.352</td>\n",
       "      <td>auto spindle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.698177</td>\n",
       "      <td>0.590</td>\n",
       "      <td>714.432</td>\n",
       "      <td>auto spindle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.947769</td>\n",
       "      <td>0.862</td>\n",
       "      <td>717.776</td>\n",
       "      <td>auto spindle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.665980</td>\n",
       "      <td>1.330</td>\n",
       "      <td>729.534</td>\n",
       "      <td>auto spindle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amplitude  Duration    Onset    Annotation\n",
       "0   7.905970     1.102  695.810  auto spindle\n",
       "1   6.556632     0.804  707.352  auto spindle\n",
       "2   6.698177     0.590  714.432  auto spindle\n",
       "3   6.947769     0.862  717.776  auto spindle\n",
       "4  10.665980     1.330  729.534  auto spindle"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
